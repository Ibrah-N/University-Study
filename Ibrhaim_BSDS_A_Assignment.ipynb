{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N3_kBFVttJeO",
        "Nwq-4c_LvSsv",
        "V-V9HrdExNBo",
        "mwLi5dHNxPcx",
        "rftIxsT8xSUY",
        "uVC0VUzB1nlC",
        "NH8-65cn2C9z",
        "8qira0rJ2LIA"
      ],
      "gpuType": "T4",
      "mount_file_id": "1fIHx7KGl_4cHkJ44lBJjSCwxLGr_0Crd",
      "authorship_tag": "ABX9TyNbVlg37LOcn1raPngKsoSx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibrah-N/University-Study/blob/main/Ibrhaim_BSDS_A_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment : 01\n",
        "- Title           : End-To-End Pipelining\n",
        "- Submitted By    : Muhammad Ibrahim\n",
        "- Submitted To    : Prof Adnan Amin\n",
        "- SubmissionDate  : 20-April-2025\n",
        "\n",
        "\n",
        "# Assignments Task List\n",
        "  1. Download Dataset\n",
        "  2. Understand The Dataset\n",
        "  3. Data Preprocessing\n",
        "    - 3.1 Data Analysis\n",
        "    - 3.2 Data Visualization\n",
        "  4. Model Building Without Feature Engineering\n",
        "  5. Apply Feature Engineering & Balancing Technique\n",
        "  6. Retrain The Same Models\n"
      ],
      "metadata": {
        "id": "UnOK4Vx38qrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Tasks"
      ],
      "metadata": {
        "id": "mUSAbQTG9LG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qPyC-PAY9Kt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8fe5b3-14a0-4b29-a8c5-7ecba067b150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Given Preprocessing Script"
      ],
      "metadata": {
        "id": "N3_kBFVttJeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, date, time, timedelta\n",
        "from Seizure_times import *\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import os\n",
        "\n",
        "CPS_seizures = []\n",
        "elec_seizures = []\n",
        "visual_seizures = []\n",
        "noc_seizures = []\n",
        "normals = []\n",
        "labels = []\n",
        "index_vect = []\n",
        "\n",
        "for patient in [15, 14, 13, 12, 11, 10]:\n",
        "\n",
        "    if patient == 10:\n",
        "        files_list=[\"Record1.edf\",\"Record2.edf\"]\n",
        "    elif patient in (15, 13, 11):\n",
        "        files_list=[\"Record1.edf\",\"Record2.edf\",\"Record3.edf\",\"Record4.edf\"]\n",
        "    elif patient in (14, 12):\n",
        "        files_list=[\"Record1.edf\",\"Record2.edf\",\"Record3.edf\"]\n",
        "\n",
        "\n",
        "    for file_id, file in enumerate(files_list):\n",
        "        file=os.path.join(\"./edfs\", \"p\"+str(patient)+\"_\"+file)\n",
        "        data = mne.io.read_raw_edf(file, preload=True)\n",
        "\n",
        "        seizure_record = []\n",
        "        index_mat = np.array([0,0])\n",
        "        ###################### Ordering channels ######################\n",
        "        if patient in (15, 14, 13, 12, 11):\n",
        "            data.drop_channels(['EEG Cz-Ref', 'EEG Pz-Ref', 'ECG EKG', 'Manual']) # see attached word file for details\n",
        "        elif patient == 10:\n",
        "            data.reorder_channels(['EEG Fp2-Ref', 'EEG Fp1-Ref', 'EEG F8-Ref', 'EEG F4-Ref', 'EEG Fz-Ref', 'EEG F3-Ref', 'EEG F7-Ref', 'EEG A2-Ref', 'EEG T4-Ref', 'EEG C4-Ref', 'EEG C3-Ref', 'EEG T3-Ref', 'EEG A1-Ref', 'EEG T6-Ref', 'EEG P4-Ref', 'EEG P3-Ref', 'EEG T5-Ref', 'EEG O2-Ref', 'EEG O1-Ref'])\n",
        "        print('#'*150)\n",
        "        print('#'*150)\n",
        "        print('#'*150)\n",
        "        #print(data.info[\"ch_names\"])\n",
        "        print('#'*150)\n",
        "        print('#'*150)\n",
        "        print('#'*150)\n",
        "        raw_data = data.get_data()      # ndarray 19 x ~5M\n",
        "        raw_data = np.array(raw_data)\n",
        "        print(raw_data.shape)\n",
        "        print('#'*150)\n",
        "        print('#'*150)\n",
        "        print('#'*150)\n",
        "        ###################################################\n",
        "        record_time = data.info['meas_date']\n",
        "        record_time = record_time.time()  #File time onset\n",
        "        record_time = datetime.combine(date.today(), record_time)\n",
        "        # get seizure time and duration\n",
        "\n",
        "        r = 'seizures_' + str(patient)\n",
        "        for i in  range(len(eval(r)[file_id+1])):  #file_id correspond to record number so here we are checking how many\n",
        "            # seizures inside a specific record\n",
        "            s_time = time(eval(r)[file_id+1][i][0], eval(r)[file_id+1][i][1], eval(r)[file_id+1][i][2])\n",
        "            seizure_duration = eval(r)[file_id+1][i][3]\n",
        "\n",
        "            print('*'*150)\n",
        "            print(\"File ID: {}, Record time:{}, Seizure time: {}, Seizure duration: {}\".format('patient_' + str(patient) + '_Record'+str(file_id+1)+'_sz'+str(i+1), record_time, s_time, seizure_duration))\n",
        "            print('*'*150)\n",
        "            print('*'*150)\n",
        "            print('*'*150)\n",
        "            diff = datetime.combine(date.today(), s_time) - record_time # assigned date does not matter\n",
        "            s_index = int( diff.total_seconds() * 500 ) # 500 sampling rate,\n",
        "            #s_index: seizure start in samples\n",
        "            s_index_end = s_index + int(seizure_duration * 500)\n",
        "            #s_index_end: seizure end in sample\n",
        "            point_duration = 1  # how many seconds is one point\n",
        "            #print(diff.days)\n",
        "            #print(diff.seconds)\n",
        "            #print(s_index)\n",
        "            #print(s_index_end)\n",
        "            print('*'*150)\n",
        "            print('*'*150)\n",
        "            print('*'*150)\n",
        "\n",
        "            # get seizure index\n",
        "\n",
        "            st = raw_data[:, s_index:s_index_end]\n",
        "\n",
        "            index_mat = np.vstack([index_mat,[s_index,s_index_end]])\n",
        "\n",
        "            print(index_mat)\n",
        "\n",
        "            if seizure_record == []:\n",
        "                seizure_record = st\n",
        "            else:\n",
        "                seizure_record = np.concatenate((seizure_record,st),axis=1)\n",
        "                #for patient 10, record2, the seizure is 305 sec, 305*500 = 152500\n",
        "\n",
        "            if len(index_mat) == 2:\n",
        "                normal_record = np.delete(raw_data, np.s_[index_mat[1,0]:index_mat[1,1]], axis=1) # remove all data corresponding to a seizure\n",
        "\n",
        "\n",
        "            elif len(index_mat) == 3:\n",
        "                x1 = np.linspace(index_mat[1,0], index_mat[1,1], index_mat[1,1]-index_mat[1,0]+1, endpoint=True)\n",
        "                x2 = np.linspace(index_mat[2,0], index_mat[2,1], index_mat[2,1]-index_mat[2,0]+1, endpoint=True)\n",
        "                y = np.concatenate((x1,x2))\n",
        "                normal_record = np.delete(raw_data, np.s_[y], axis=1)\n",
        "\n",
        "            elif len(index_mat) == 4:\n",
        "                x1 = np.linspace(index_mat[1,0], index_mat[1,1], index_mat[1,1]-index_mat[1,0]+1, endpoint=True)\n",
        "                x2 = np.linspace(index_mat[2,0], index_mat[2,1], index_mat[2,1]-index_mat[2,0]+1, endpoint=True)\n",
        "                x3 = np.linspace(index_mat[3,0], index_mat[3,1], index_mat[3,1]-index_mat[2,0]+1, endpoint=True)\n",
        "                y = np.concatenate((x1,x2,x3))\n",
        "                normal_record = np.delete(raw_data, np.s_[y], axis=1)\n",
        "\n",
        "            elif len(index_mat) == 5:\n",
        "                x1 = np.linspace(index_mat[1,0], index_mat[1,1], index_mat[1,1]-index_mat[1,0]+1, endpoint=True)\n",
        "                x2 = np.linspace(index_mat[2,0], index_mat[2,1], index_mat[2,1]-index_mat[2,0]+1, endpoint=True)\n",
        "                x3 = np.linspace(index_mat[3,0], index_mat[3,1], index_mat[3,1]-index_mat[2,0]+1, endpoint=True)\n",
        "                x4 = np.linspace(index_mat[4,0], index_mat[4,1], index_mat[4,1]-index_mat[2,0]+1, endpoint=True)\n",
        "                y = np.concatenate((x1,x2,x3,x4))\n",
        "                normal_record = np.delete(raw_data, np.s_[y], axis=1)\n",
        "\n",
        "            print(seizure_record.shape)\n",
        "            print('*'*150)\n",
        "            print('*'*150)\n",
        "            print('*'*150)\n",
        "\n",
        "            if patient == 10:\n",
        "                for i in range(seizure_duration):\n",
        "                    data_point = seizure_record[:, i * 500:(i + 1) * 500]  #this will be 19x500\n",
        "                    elec_seizures.append(data_point)  # this will a list of for example 305 matrix of size 19x500\n",
        "            elif patient ==13 and file_id<3:\n",
        "                for i in range(seizure_duration):\n",
        "                    data_point = seizure_record[:, i * 500:(i + 1) * 500]  #this will be 19x500\n",
        "                    noc_seizures.append(data_point)  # this will a list of for example 305 matrix of size 19x500\n",
        "            else:\n",
        "                for i in range(seizure_duration):           # i is the one second interval\n",
        "                    data_point = seizure_record[:, i*500:(i+1)*500]\n",
        "                    CPS_seizures.append(data_point)         # CPS is set of points, each of which is 19x500\n",
        "\n",
        "            # normals\n",
        "            for i in range(seizure_duration): #take same number of normal points\n",
        "                data_point = normal_record[:, i*500:(i+1)*500]\n",
        "                normals.append(data_point)\n",
        "\n",
        "\n",
        "# transpose second and third dim\n",
        "CPS_seizures = np.array(CPS_seizures)\n",
        "scaler = np.amax(abs(CPS_seizures))\n",
        "CPS_seizures = CPS_seizures/scaler\n",
        "\n",
        "elec_seizures = np.array(elec_seizures)\n",
        "scaler = np.amax(abs(elec_seizures))\n",
        "elec_seizures = elec_seizures/scaler\n",
        "\n",
        "noc_seizures = np.array(noc_seizures)\n",
        "scaler = np.amax(abs(noc_seizures))\n",
        "noc_seizures = noc_seizures/scaler\n",
        "\n",
        "normals = np.array(normals)\n",
        "scaler = np.amax(abs(normals))\n",
        "normals = normals/scaler\n",
        "\n",
        "# plt.figure('Normal')\n",
        "# plt.plot(normals[100].T)\n",
        "# plt.figure('Seizure')\n",
        "# plt.plot(seizures[100].T)\n",
        "# plt.show()\n",
        "#\n",
        "\n",
        "x = np.vstack((noc_seizures, elec_seizures, CPS_seizures, normals)) #trying to construct the x #points x 19 x 500\n",
        "\n",
        "# below order matters\n",
        "labels.extend([3 for i in range(len(noc_seizures))])\n",
        "labels.extend([2 for i in range(len(elec_seizures))])\n",
        "labels.extend([1 for i in range(len(CPS_seizures))])\n",
        "labels.extend([0 for i in range(len(normals))])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, labels, test_size=0.1, random_state=1)\n",
        "\n",
        "np.save(\"./data_aligned/x_train\", x_train)\n",
        "np.save(\"./data_aligned/x_test\", x_test)\n",
        "np.save(\"./data_aligned/y_train\", y_train)\n",
        "np.save(\"./data_aligned/y_test\", y_test)\n",
        "\n",
        "##############################################################################\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "matrix = np.load('./data_aligned/x_test.npy')\n",
        "scipy.io.savemat('./data_aligned/x_test.mat', dict(x_test=matrix))\n",
        "##############################################################################\n",
        "matrix2 = np.load('./data_aligned/x_train.npy')\n",
        "scipy.io.savemat('./data_aligned/x_train.mat', dict(x_train=matrix2))\n",
        "##############################################################################\n",
        "matrixy = np.load('./data_aligned/y_test.npy')\n",
        "scipy.io.savemat('./data_aligned/y_test.mat', dict(y_test=matrixy))\n",
        "##############################################################################\n",
        "matrixy2 = np.load('./data_aligned/y_train.npy')\n",
        "scipy.io.savemat('./data_aligned/y_train.mat', dict(y_train=matrixy2))"
      ],
      "metadata": {
        "id": "VfOfriNws55_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZ6kz5mSt49u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "id": "Nwq-4c_LvSsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "PrVNKxEM9Kbt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/data_aligned /content/\n",
        "\n",
        "print(os.listdir(\"/content/data_aligned/\"))"
      ],
      "metadata": {
        "id": "0swzxgNxHG50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e11aaa9-c71b-4783-b4ff-3fe0b6837ac3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x_test.npy', 'x_train.npy', 'y_train.npy', 'y_test.npy', 'x_test.mat', 'x_train.mat', 'y_test.mat', 'y_train.mat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.load(\"/content/data_aligned/x_train.npy\")\n",
        "y_train = np.load(\"/content/data_aligned/y_train.npy\")\n",
        "x_test = np.load(\"/content/data_aligned/x_test.npy\")\n",
        "y_test = np.load(\"/content/data_aligned/y_test.npy\")"
      ],
      "metadata": {
        "id": "TXAly_UG9KaI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36udv5xKt_NH",
        "outputId": "9d5047e0-ed16-4cec-e37e-800a4a726dfa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6232, 19, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "from sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input,Dense,Dropout,Normalization,BatchNormalization,LayerNormalization\n",
        "from tensorflow.keras.layers import LSTM,GRU,Embedding,Reshape\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,Flatten\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam,Adamax\n",
        "from tensorflow.keras.regularizers import l1,l2"
      ],
      "metadata": {
        "id": "-YH1xKWhvVed"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allocate 10% of train data to validation set\n",
        "x_val = x_train[-701:,:,:]\n",
        "y_val = y_train[-701:]\n",
        "x_train = x_train[:-701,:,:]\n",
        "y_train = y_train[:-701]"
      ],
      "metadata": {
        "id": "tAUUjirGvXQx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes of train, validation, and test sets\n",
        "print(f'x_train shape:{x_train.shape},\\\n",
        "      \\ny_train shape:{y_train.shape},\\\n",
        "      \\nx_val shape:{x_val.shape},\\\n",
        "      \\ny_val shape:{y_val.shape},\\\n",
        "      \\nx_test shape:{x_test.shape},\\\n",
        "      \\ny_test shape:{y_test.shape}\\n')\n",
        "\n",
        "# List the classes that exist within the dataset\n",
        "print(f'Classes: {np.unique(y_train)}')\n",
        "print('Seizure Types: \\n1: for Complex Partial Seizures, 2: for Electrographic Seizures,\\\n",
        "       \\n3: for Video-detected Seizures with no visual change over EEG, 0: for Normal data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgPjLwg8vXMP",
        "outputId": "78f3d89d-1f74-417a-ac81-92b5dc5ee16e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:(5531, 19, 500),      \n",
            "y_train shape:(5531,),      \n",
            "x_val shape:(701, 19, 500),      \n",
            "y_val shape:(701,),      \n",
            "x_test shape:(1558, 19, 500),      \n",
            "y_test shape:(1558,)\n",
            "\n",
            "Classes: [0 1 2 3]\n",
            "Seizure Types: \n",
            "1: for Complex Partial Seizures, 2: for Electrographic Seizures,       \n",
            "3: for Video-detected Seizures with no visual change over EEG, 0: for Normal data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "V-V9HrdExNBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model : LSTM"
      ],
      "metadata": {
        "id": "mwLi5dHNxPcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the neural network model\n",
        "\n",
        "tf.random.set_seed(1234)                                   # For consistent results\n",
        "\n",
        "model = Sequential([\n",
        "    Input((x_train.shape[1],x_train.shape[2],1)),          # Input layer for EEG data: (electrodes, time points, channels)\n",
        "\n",
        "    Conv2D(8, (1,250), activation='relu', padding='same'), # Kernel length set to half the sampling rate of the data\n",
        "    BatchNormalization(),                                  # Normalize activations to stabilize training\n",
        "    AveragePooling2D((1,4)),                               # Downsample feature maps to reduce computational complexity and prevent overfitting\n",
        "    Dropout(0.2),                                          # Include dropout to prevent overfitting\n",
        "\n",
        "    Conv2D(16,(1,62), activation='relu', padding='same'),  # Additional convolutional layers to capture hierarchical features\n",
        "    BatchNormalization(),\n",
        "    AveragePooling2D((1,2)),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(32,(1,31), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    AveragePooling2D((1,2)),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Reshape((x_train.shape[1],-1)),                        # Reshape to prepare for LSTM layer\n",
        "    LSTM(128, activation='tanh'),\n",
        "\n",
        "    Dense(4)\n",
        "])"
      ],
      "metadata": {
        "id": "y2zBIwIPvXJx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit the model\n",
        "\n",
        "model.compile(\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = Adamax(0.001),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x_train,y_train,\n",
        "    epochs = 20,\n",
        "    verbose = 1,\n",
        "    validation_data=(x_val,y_val)\n",
        ")\n",
        "\n",
        "train_loss, train_accuracy = model.evaluate(x_train, y_train)\n",
        "val_loss, val_accuracy = model.evaluate(x_val, y_val)\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f'Train Loss: {train_loss:.2f}, Validation Loss: {val_loss:.2f},  Test Loss: {test_loss:.2f}')\n",
        "print(f'Train Accuracy: {train_accuracy*100:.2f}%, Validation Accuracy: {val_accuracy*100:.2f}%, Test Accuracy: {test_accuracy*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMMXii-FwcXi",
        "outputId": "f0c75308-3e96-426b-c041-39f4ef15c132"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.6876 - loss: 0.8082 - val_accuracy: 0.4950 - val_loss: 1.3881\n",
            "Epoch 2/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.8105 - loss: 0.5152 - val_accuracy: 0.1170 - val_loss: 1.4843\n",
            "Epoch 3/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.8619 - loss: 0.3769 - val_accuracy: 0.4051 - val_loss: 1.1780\n",
            "Epoch 4/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.8926 - loss: 0.2961 - val_accuracy: 0.4351 - val_loss: 1.3384\n",
            "Epoch 5/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.9033 - loss: 0.2511 - val_accuracy: 0.8673 - val_loss: 0.3206\n",
            "Epoch 6/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.9202 - loss: 0.2141 - val_accuracy: 0.8930 - val_loss: 0.2628\n",
            "Epoch 7/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.9255 - loss: 0.1860 - val_accuracy: 0.8830 - val_loss: 0.2698\n",
            "Epoch 8/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.9396 - loss: 0.1574 - val_accuracy: 0.9173 - val_loss: 0.2138\n",
            "Epoch 9/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9524 - loss: 0.1337 - val_accuracy: 0.8859 - val_loss: 0.2547\n",
            "Epoch 10/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9607 - loss: 0.1104 - val_accuracy: 0.9030 - val_loss: 0.2590\n",
            "Epoch 11/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.9663 - loss: 0.0954 - val_accuracy: 0.8959 - val_loss: 0.2688\n",
            "Epoch 12/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9689 - loss: 0.0850 - val_accuracy: 0.9258 - val_loss: 0.2221\n",
            "Epoch 13/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.9729 - loss: 0.0779 - val_accuracy: 0.9287 - val_loss: 0.2440\n",
            "Epoch 14/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9794 - loss: 0.0667 - val_accuracy: 0.9387 - val_loss: 0.1958\n",
            "Epoch 15/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9820 - loss: 0.0606 - val_accuracy: 0.9401 - val_loss: 0.2133\n",
            "Epoch 16/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.9847 - loss: 0.0501 - val_accuracy: 0.9287 - val_loss: 0.2184\n",
            "Epoch 17/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.9836 - loss: 0.0509 - val_accuracy: 0.9244 - val_loss: 0.2777\n",
            "Epoch 18/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.9848 - loss: 0.0470 - val_accuracy: 0.9472 - val_loss: 0.2026\n",
            "Epoch 19/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.9915 - loss: 0.0347 - val_accuracy: 0.9372 - val_loss: 0.2346\n",
            "Epoch 20/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9892 - loss: 0.0352 - val_accuracy: 0.9372 - val_loss: 0.2300\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9735 - loss: 0.0678\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9301 - loss: 0.2400\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9352 - loss: 0.2634\n",
            "Train Loss: 0.07, Validation Loss: 0.23,  Test Loss: 0.27\n",
            "Train Accuracy: 97.43%, Validation Accuracy: 93.72%, Test Accuracy: 93.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Saving the model weights and archeture\n",
        "model.save(\"model_01.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_mmNctxxmqZ",
        "outputId": "3a7332a5-8907-40ab-b3e4-510625586da7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJV76g3cwops"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: 1D CNN + GRU Hybrid"
      ],
      "metadata": {
        "id": "rftIxsT8xSUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D\n",
        "\n",
        "model2 = Sequential([\n",
        "    Input((x_train.shape[1], x_train.shape[2])),\n",
        "\n",
        "    Conv1D(64, kernel_size=7, padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv1D(128, kernel_size=5, padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    GRU(64, return_sequences=False),\n",
        "    Dense(4)\n",
        "])\n"
      ],
      "metadata": {
        "id": "1RQLeGeNxVTl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=Adam(0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model2.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=20,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iRtEdJpxVRc",
        "outputId": "736aa86c-e13d-4e8b-87bc-fc978b49a098"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.6451 - loss: 0.9069 - val_accuracy: 0.5321 - val_loss: 1.0220\n",
            "Epoch 2/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7392 - loss: 0.6580 - val_accuracy: 0.6605 - val_loss: 0.8256\n",
            "Epoch 3/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7890 - loss: 0.5506 - val_accuracy: 0.7532 - val_loss: 0.5993\n",
            "Epoch 4/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8346 - loss: 0.4494 - val_accuracy: 0.8074 - val_loss: 0.5040\n",
            "Epoch 5/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8605 - loss: 0.3911 - val_accuracy: 0.8245 - val_loss: 0.5034\n",
            "Epoch 6/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8770 - loss: 0.3364 - val_accuracy: 0.8245 - val_loss: 0.5219\n",
            "Epoch 7/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8868 - loss: 0.3004 - val_accuracy: 0.8060 - val_loss: 0.5692\n",
            "Epoch 8/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9025 - loss: 0.2686 - val_accuracy: 0.8046 - val_loss: 0.5756\n",
            "Epoch 9/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9072 - loss: 0.2538 - val_accuracy: 0.8160 - val_loss: 0.5785\n",
            "Epoch 10/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9201 - loss: 0.2209 - val_accuracy: 0.8288 - val_loss: 0.5377\n",
            "Epoch 11/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9197 - loss: 0.2115 - val_accuracy: 0.8174 - val_loss: 0.6084\n",
            "Epoch 12/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9256 - loss: 0.2036 - val_accuracy: 0.8203 - val_loss: 0.5798\n",
            "Epoch 13/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9224 - loss: 0.1888 - val_accuracy: 0.8174 - val_loss: 0.5550\n",
            "Epoch 14/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9388 - loss: 0.1634 - val_accuracy: 0.8188 - val_loss: 0.5716\n",
            "Epoch 15/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9423 - loss: 0.1587 - val_accuracy: 0.8274 - val_loss: 0.5620\n",
            "Epoch 16/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9491 - loss: 0.1456 - val_accuracy: 0.8474 - val_loss: 0.5068\n",
            "Epoch 17/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9495 - loss: 0.1383 - val_accuracy: 0.8402 - val_loss: 0.5454\n",
            "Epoch 18/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9451 - loss: 0.1423 - val_accuracy: 0.8288 - val_loss: 0.5912\n",
            "Epoch 19/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9523 - loss: 0.1319 - val_accuracy: 0.8274 - val_loss: 0.5984\n",
            "Epoch 20/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9607 - loss: 0.1103 - val_accuracy: 0.8402 - val_loss: 0.6042\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e5fde2b8750>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## saving the models\n",
        "model2.save(\"model_02.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpyzP6y_xVPJ",
        "outputId": "d4c8391d-e1f4-4dfe-90f3-d3bcbcb46bac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: Transformer-style Attention Model"
      ],
      "metadata": {
        "id": "uVC0VUzB1nlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add\n",
        "\n",
        "input_layer = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
        "\n",
        "# Positional Encoding not added for simplicity – you can try later\n",
        "attn_output = MultiHeadAttention(num_heads=4, key_dim=32)(input_layer, input_layer)\n",
        "attn_output = Dropout(0.2)(attn_output)\n",
        "attn_output = Add()([input_layer, attn_output])  # Residual Connection\n",
        "attn_output = LayerNormalization()(attn_output)\n",
        "\n",
        "# Feed Forward Network\n",
        "ffn = Dense(128, activation='relu')(attn_output)\n",
        "ffn = Dropout(0.3)(ffn)\n",
        "ffn = Dense(64, activation='relu')(ffn)\n",
        "ffn = Dropout(0.2)(ffn)\n",
        "\n",
        "flattened = Flatten()(ffn)\n",
        "output_layer = Dense(4)(flattened)\n",
        "\n",
        "model3 = tf.keras.Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "BFo-ZcQnxVMM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=Adam(0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model3.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=20,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txXRnEBT1nD5",
        "outputId": "b33e43bc-ac60-4575-ecb3-288040f94758"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - accuracy: 0.6271 - loss: 0.8641 - val_accuracy: 0.7989 - val_loss: 0.5117\n",
            "Epoch 2/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8009 - loss: 0.5110 - val_accuracy: 0.8345 - val_loss: 0.4511\n",
            "Epoch 3/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8387 - loss: 0.4382 - val_accuracy: 0.8359 - val_loss: 0.4487\n",
            "Epoch 4/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8581 - loss: 0.4031 - val_accuracy: 0.8488 - val_loss: 0.4181\n",
            "Epoch 5/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8582 - loss: 0.3658 - val_accuracy: 0.8459 - val_loss: 0.4188\n",
            "Epoch 6/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8573 - loss: 0.3726 - val_accuracy: 0.8631 - val_loss: 0.3813\n",
            "Epoch 7/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8838 - loss: 0.3202 - val_accuracy: 0.8545 - val_loss: 0.4021\n",
            "Epoch 8/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8765 - loss: 0.3311 - val_accuracy: 0.8559 - val_loss: 0.4143\n",
            "Epoch 9/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8815 - loss: 0.3107 - val_accuracy: 0.8459 - val_loss: 0.4115\n",
            "Epoch 10/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8852 - loss: 0.3108 - val_accuracy: 0.8616 - val_loss: 0.3825\n",
            "Epoch 11/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8857 - loss: 0.3137 - val_accuracy: 0.8588 - val_loss: 0.4107\n",
            "Epoch 12/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8945 - loss: 0.2815 - val_accuracy: 0.8673 - val_loss: 0.3729\n",
            "Epoch 13/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8987 - loss: 0.2623 - val_accuracy: 0.8616 - val_loss: 0.3962\n",
            "Epoch 14/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9011 - loss: 0.2723 - val_accuracy: 0.8673 - val_loss: 0.4318\n",
            "Epoch 15/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8819 - loss: 0.3143 - val_accuracy: 0.8588 - val_loss: 0.4288\n",
            "Epoch 16/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9008 - loss: 0.2602 - val_accuracy: 0.8474 - val_loss: 0.4380\n",
            "Epoch 17/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8948 - loss: 0.2767 - val_accuracy: 0.8631 - val_loss: 0.3774\n",
            "Epoch 18/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9051 - loss: 0.2524 - val_accuracy: 0.8559 - val_loss: 0.3557\n",
            "Epoch 19/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9035 - loss: 0.2536 - val_accuracy: 0.8573 - val_loss: 0.3978\n",
            "Epoch 20/20\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9057 - loss: 0.2516 - val_accuracy: 0.8631 - val_loss: 0.3924\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e5fdc372d50>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## saving the model\n",
        "model3.save(\"model3.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEYFY3-o1_fv",
        "outputId": "128d3d40-b4e5-46b0-b628-d717209976ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Qsk5AiX1_dQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Of Models"
      ],
      "metadata": {
        "id": "htUAKBZDwpRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model : LSTM"
      ],
      "metadata": {
        "id": "NH8-65cn2C9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with other metrics\n",
        "\n",
        "# Train data\n",
        "y_train_predict_ = model.predict(x_train)\n",
        "y_train_predict = np.argmax(tf.nn.softmax(y_train_predict_), axis=1)\n",
        "\n",
        "# Test data\n",
        "y_test_predict_ = model.predict(x_test)\n",
        "y_test_predict = np.argmax(tf.nn.softmax(y_test_predict_), axis=1)\n",
        "\n",
        "# Define the metrics and corresponding values\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "train_values = [accuracy_score(y_train, y_train_predict) * 100,\n",
        "                precision_score(y_train, y_train_predict, average='macro') * 100,\n",
        "                recall_score(y_train, y_train_predict, average='macro') * 100,\n",
        "                f1_score(y_train, y_train_predict, average='macro') * 100]\n",
        "test_values = [accuracy_score(y_test, y_test_predict) * 100,\n",
        "               precision_score(y_test, y_test_predict, average='macro') * 100,\n",
        "               recall_score(y_test, y_test_predict, average='macro') * 100,\n",
        "               f1_score(y_test, y_test_predict, average='macro') * 100]\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Metric:': metrics,\n",
        "    'Train:': train_values,\n",
        "    'Test:': test_values\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Print DataFrame\n",
        "print(\"\\nTrain and Test Data Results -\")\n",
        "print(results_df.round(2).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDQ_xCZx1amy",
        "outputId": "ecef3f0a-6709-4ca6-c5f3-8c5ad7194c44"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
            "\n",
            "Train and Test Data Results -\n",
            "  Metric:  Train:  Test:\n",
            " Accuracy   97.43  93.07\n",
            "Precision   97.85  94.63\n",
            "   Recall   98.02  92.85\n",
            " F1 Score   97.90  93.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: 1D CNN + GRU Hybrid"
      ],
      "metadata": {
        "id": "8qira0rJ2LIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with other metrics\n",
        "\n",
        "# Train data\n",
        "y_train_predict_ = model2.predict(x_train)\n",
        "y_train_predict = np.argmax(tf.nn.softmax(y_train_predict_), axis=1)\n",
        "\n",
        "# Test data\n",
        "y_test_predict_ = model2.predict(x_test)\n",
        "y_test_predict = np.argmax(tf.nn.softmax(y_test_predict_), axis=1)\n",
        "\n",
        "# Define the metrics and corresponding values\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "train_values = [accuracy_score(y_train, y_train_predict) * 100,\n",
        "                precision_score(y_train, y_train_predict, average='macro') * 100,\n",
        "                recall_score(y_train, y_train_predict, average='macro') * 100,\n",
        "                f1_score(y_train, y_train_predict, average='macro') * 100]\n",
        "test_values = [accuracy_score(y_test, y_test_predict) * 100,\n",
        "               precision_score(y_test, y_test_predict, average='macro') * 100,\n",
        "               recall_score(y_test, y_test_predict, average='macro') * 100,\n",
        "               f1_score(y_test, y_test_predict, average='macro') * 100]\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Metric:': metrics,\n",
        "    'Train:': train_values,\n",
        "    'Test:': test_values\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Print DataFrame\n",
        "print(\"\\nTrain and Test Data Results -\")\n",
        "print(results_df.round(2).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCkODmGQvXHd",
        "outputId": "bbc573f1-0cef-47f8-9b0e-e301be620514"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\n",
            "Train and Test Data Results -\n",
            "  Metric:  Train:  Test:\n",
            " Accuracy   94.12  84.53\n",
            "Precision   92.99  85.39\n",
            "   Recall   89.58  71.83\n",
            " F1 Score   90.49  73.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: Transformer-style Attention Model"
      ],
      "metadata": {
        "id": "tQRIolYg2PIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with other metrics\n",
        "\n",
        "# Train data\n",
        "y_train_predict_ = model3.predict(x_train)\n",
        "y_train_predict = np.argmax(tf.nn.softmax(y_train_predict_), axis=1)\n",
        "\n",
        "# Test data\n",
        "y_test_predict_ = model3.predict(x_test)\n",
        "y_test_predict = np.argmax(tf.nn.softmax(y_test_predict_), axis=1)\n",
        "\n",
        "# Define the metrics and corresponding values\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "train_values = [accuracy_score(y_train, y_train_predict) * 100,\n",
        "                precision_score(y_train, y_train_predict, average='macro') * 100,\n",
        "                recall_score(y_train, y_train_predict, average='macro') * 100,\n",
        "                f1_score(y_train, y_train_predict, average='macro') * 100]\n",
        "test_values = [accuracy_score(y_test, y_test_predict) * 100,\n",
        "               precision_score(y_test, y_test_predict, average='macro') * 100,\n",
        "               recall_score(y_test, y_test_predict, average='macro') * 100,\n",
        "               f1_score(y_test, y_test_predict, average='macro') * 100]\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Metric:': metrics,\n",
        "    'Train:': train_values,\n",
        "    'Test:': test_values\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Print DataFrame\n",
        "print(\"\\nTrain and Test Data Results -\")\n",
        "print(results_df.round(2).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLOW395IwTZG",
        "outputId": "ad10a5cc-bbb0-4566-ea57-950e0eb5da7c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
            "\n",
            "Train and Test Data Results -\n",
            "  Metric:  Train:  Test:\n",
            " Accuracy   92.95  86.39\n",
            "Precision   94.36  88.94\n",
            "   Recall   94.64  80.66\n",
            " F1 Score   94.42  83.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0kYf_j0vXFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-H_f2QKvXC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/model_01.h5 /content/drive/MyDrive/AdnanAminAssignment\n",
        "!cp /content/model_02.h5 /content/drive/MyDrive/AdnanAminAssignment\n",
        "!cp /content/model_03.h5 /content/drive/MyDrive/AdnanAminAssignment"
      ],
      "metadata": {
        "id": "hBBlL1nAvXAh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Xfmj2bvvW-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZ1BGWAGvW7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_DjaBag8l2g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpuKESae3DuN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}